{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorflow InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Class and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul(x, W) + b\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "for _ in range(1000):\n",
    "        batch = mnist.train.next_batch(128)\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Densely Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# training step\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation metric\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        batch = mnist.train.next_batch(64)\n",
    "        if i % 100 == 0 :\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x: batch[0],\n",
    "                y_: batch[1],\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={\n",
    "            x: batch[0],\n",
    "            y_: batch[1],\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "    # Evaluate\n",
    "    print(\"test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images,\n",
    "        y_: mnist.test.labels,\n",
    "        keep_prob: 1.0\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Hierarchical Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "class RandomTreeEncoder:\n",
    "    def __init__(self, num_trees=10):\n",
    "        self.num_trees = num_trees\n",
    "        self.encoders = []\n",
    "        \n",
    "    def build_encoders(self, training_set, labels=None):\n",
    "        self.encoders = []\n",
    "        for i in range(self.num_trees):\n",
    "            encoder = RandomTreesEmbedding(\n",
    "                n_estimators=1,\n",
    "                max_depth=None#,\n",
    "#                 min_weight_fraction_leaf=0.00005\n",
    "#                 min_impurity_decrease=0.01\n",
    "            )\n",
    "            encoder.fit(training_set)\n",
    "            self.encoders.append(encoder)\n",
    "        if type(labels) is np.ndarray:\n",
    "            for label in np.unique(labels):\n",
    "                for i in range(self.num_trees):\n",
    "                    # the label\n",
    "                    label_set = training_set[labels==label]\n",
    "                    encoder = RandomTreesEmbedding(\n",
    "                        n_estimators=1,\n",
    "                        max_depth=None\n",
    "                    )\n",
    "                    encoder.fit(label_set)\n",
    "                    self.encoders.append(encoder)\n",
    "                    # other labels\n",
    "                    label_set = training_set[labels!=label]\n",
    "                    encoder = RandomTreesEmbedding(\n",
    "                        n_estimators=1,\n",
    "                        max_depth=None\n",
    "                    )\n",
    "                    encoder.fit(label_set)\n",
    "                    self.encoders.append(encoder)\n",
    "\n",
    "    def encode_features(self, features):\n",
    "        \"\"\"\n",
    "        Encodes a dataframe of features using the set of encoders\n",
    "        trained on the training set.\n",
    "        \"\"\"\n",
    "        encodings = pd.DataFrame()\n",
    "        for i in range(len(self.encoders)):\n",
    "            encoder = self.encoders[i]\n",
    "            encoding = encoder.transform(features)\n",
    "            encodings[str(i)] = encoding.nonzero()[1]\n",
    "        return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = np.unique(Y)\n",
    "# for label in labels:\n",
    "#     print(np.shape(X[Y==label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.train.images\n",
    "Y = np.argmax(mnist.train.labels, 1)\n",
    "x_prediction = mnist.test.images\n",
    "y_test = np.argmax(mnist.test.labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rte.encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rte = RandomTreeEncoder(30)\n",
    "rte.build_encoders(X, Y)\n",
    "X_encodings = rte.encode_features(X)\n",
    "Pred_encodings = rte.encode_features(x_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_encodings.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encodings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train-test split evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = XGBClassifier(max_depth=20, objective=\"multi:softmax\")\n",
    "kfold = KFold(n_splits=3, random_state=7)\n",
    "results = cross_val_score(model, X_encodings, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "# 96.1% 700 features min_impurity decrease=0.0\n",
    "# Accuracy: 94.85% (0.03%) 100 features min_impurity decrease=0.0\n",
    "# Wall time: 4min 17s\n",
    "# Accuracy: 89.61% (0.14%)\n",
    "# Wall time: 2min 3s\n",
    "# Accuracy: 95.17% (0.19%) 100 features min_i_d=0.0 & softmax\n",
    "# Wall time: 4min 12s\n",
    "# Accuracy: 96.15% (0.03%) 700 features & softmax\n",
    "# Wall time: 25min 35s\n",
    "# Accuracy: 96.03% (0.04%) 700 features & softmax & max_depth 20 instead of 10\n",
    "# Wall time: 36min 19s\n",
    "# Accuracy: 95.47% (0.13%) 20 + 200\n",
    "# Wall time: 9min 34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = XGBClassifier(max_depth=20, objective=\"multi:softmax\")\n",
    "# compute predictions\n",
    "model.fit(X_encodings, Y)\n",
    "# y_prediction = model.predict_proba(Pred_encodings)\n",
    "y_prediction = model.predict(Pred_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_prediction)\n",
    "# 0.95540000000000003 with 100 trees\n",
    "# 0.96330000000000005 with 200 trees\n",
    "# 0.96830000000000005 with 700 trees\n",
    "# 0.90659999999999996 with 100 trees and min_weight_fraction_leaf=0.1\n",
    "# 0.95689999999999997 with 100 trees, softmax and min_weight_fraction_leaf=0.00005\n",
    "# 0.96140000000000003 with 100 trees & softmax\n",
    "# 0.96689999999999998 with 700 trees & softmax\n",
    "# 0.96450000000000002 with 700 trees & softmax & max_depth 20\n",
    "# 0.96299999999999997 with 20 + 200 trees & max_depth 20\n",
    "# 0.96679999999999999 with 100 + 1000 trees & max_depth 20\n",
    "# 0.96479999999999999 with 20 + 400 trees & max_depth 20\n",
    "# 0.9647              with 20 + 400 trees & max_depth 10\n",
    "# 0.9446              with 20 + 400 trees & max_depth 3\n",
    "# 0.96919999999999995 with 100 + 2000 trees & max_depth 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = np.argsort(model.feature_importances_)[::-1]\n",
    "print(\"Top features: {}\".format(top_features))\n",
    "print(\"Top Scores: {}\".format(model.feature_importances_[top_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_encodings(encodings):\n",
    "    return (encodings / 100).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(bin_encodings(X_encodings), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict(bin_encodings(Pred_encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = np.argsort(model.feature_importances_)[::-1]\n",
    "print(\"Top features: {}\".format(top_features))\n",
    "print(\"Top Scores: {}\".format(model.feature_importances_[top_features]))\n",
    "# Top features: [2 1 0 3 6 4 5 8 9 7] <= should be like in almost increasing order\n",
    "# since we feed top 10 features in decreasing order of importances\n",
    "# Top Scores: [ 0.10375565  0.10359149  0.10310668  0.10089635  0.09991525  0.09964803\n",
    "#   0.09869365  0.09841879  0.09672765  0.09524646]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_prediction)\n",
    "# 0.95509999999999995 with 100 trees\n",
    "# 0.89259999999999995 with random 10 trees\n",
    "# 0.90459999999999996 with best 10 trees given by feature importances\n",
    "# 0.96789999999999998 with 700 trees and bin encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [str(i) for i in range(10)]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [str(c) for c in top_features[:10]]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(bin_encodings(X_encodings[columns]), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict(bin_encodings(Pred_encodings[columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = np.argsort(model.feature_importances_)[::-1]\n",
    "print(\"Top features: {}\".format(top_features))\n",
    "print(\"Top Scores: {}\".format(model.feature_importances_[top_features]))\n",
    "# Top features: [2 1 0 3 6 4 5 8 9 7] <= should be like in almost increasing order\n",
    "# since we feed top 10 features in decreasing order of importances\n",
    "# Top Scores: [ 0.10375565  0.10359149  0.10310668  0.10089635  0.09991525  0.09964803\n",
    "#   0.09869365  0.09841879  0.09672765  0.09524646]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_prediction)\n",
    "# 0.95509999999999995 with 100 trees\n",
    "# 0.89259999999999995 with random 10 trees\n",
    "# 0.90459999999999996 with best 10 trees given by feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST with no Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = XGBClassifier(max_depth=5, objective=\"multi:softmax\")\n",
    "model.fit(X, Y)\n",
    "y_prediction = model.predict(x_prediction)\n",
    "# Wall time: 6min 57s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_prediction)\n",
    "#  0.97109999999999996\n",
    "#  0.97160000000000002 with 20 max_depth\n",
    "#  0.9637 with 5 max_depth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
